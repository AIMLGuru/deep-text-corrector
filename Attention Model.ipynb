{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47c6f5c4",
   "metadata": {
    "id": "47c6f5c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,RNN,Flatten\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "from IPython.display import Image\n",
    "import io\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3d7189",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae3d7189",
    "outputId": "c6171189-aa98-46e5-9787-d63081f5463a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#os.chdir('/content/drive/MyDrive/Deep Text Corrector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "-pIopESOD6Yi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-pIopESOD6Yi",
    "outputId": "08773160-72ca-45b1-9bc6-abbc8a3b30da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Bazinga\\\\AAIC\\\\Deep Text Corrector'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f2580b",
   "metadata": {
    "id": "b8f2580b"
   },
   "outputs": [],
   "source": [
    "f = open('./data/perturbated_text_embed_matrix.pkl','rb')\n",
    "perturbated_text_embed_matrix = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/text_embed_matrix.pkl','rb')\n",
    "text_embed_matrix = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/perturbated_text_train.pkl','rb')\n",
    "perturbated_text_train = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/text_inp_train.pkl','rb')\n",
    "text_inp_train = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/text_out_train.pkl','rb')\n",
    "text_out_train = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/perturbated_text_tokernizer_index.pkl','rb')\n",
    "perturbated_text_tokernizer_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/text_inp_tokernizer_word_index.pkl','rb')\n",
    "text_inp_tokernizer_word_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/text_inp_tokernizer.pkl','rb')\n",
    "text_inp_tokernizer = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/train_data.pkl','rb')\n",
    "train_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/test_data.pkl','rb')\n",
    "test_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./data/validation_data.pkl','rb')\n",
    "validation_data = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d43666",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7d43666",
    "outputId": "14fafd04-16eb-4334-aa41-597fa3779004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163668\n",
      "163668\n",
      "163668\n",
      "(163668, 20)\n",
      "(163668, 20)\n",
      "(163668, 20)\n",
      "34594\n",
      "34593\n"
     ]
    }
   ],
   "source": [
    "print(len(perturbated_text_train))\n",
    "print(len(text_inp_train))\n",
    "print(len(text_out_train))\n",
    "\n",
    "print(perturbated_text_train.shape)\n",
    "print(text_inp_train.shape)\n",
    "print(text_out_train.shape)\n",
    "\n",
    "print(len(text_embed_matrix))\n",
    "print(len(perturbated_text_embed_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c76dc04e",
   "metadata": {
    "id": "c76dc04e"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = Embedding(input_dim=inp_vocab_size, output_dim=300, input_length=input_length,\n",
    "                           mask_zero=True,name=\"embedding_layer_encoder\", weights=[perturbated_text_embed_matrix], trainable=False)\n",
    "        self.lstmcell = tf.keras.layers.LSTMCell(lstm_size)\n",
    "        self.encoder_lstm = RNN(self.lstmcell,return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "\n",
    "        output1 = self.embedding(input_sequence)\n",
    "        mask = self.embedding.compute_mask(input_sequence)\n",
    "        enco_output, enco_state_h, enco_state_c = self.encoder_lstm(output1, initial_state=states,mask=mask)\n",
    "        return enco_output, enco_state_h, enco_state_c\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "\n",
    "        initial_hidden_state = tf.zeros([batch_size,self.lstm_size])\n",
    "        initial_cell_state = tf.zeros([batch_size,self.lstm_size])\n",
    "        \n",
    "        return [initial_hidden_state,initial_cell_state]\n",
    "    \n",
    "############################## Decoder class #############################################################\n",
    "    \n",
    "# code reference for concat scoing scoring function from https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,scoring_function,att_units):\n",
    "\n",
    "\n",
    "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "    super().__init__()\n",
    "    self.scoring_function = scoring_function\n",
    "    \n",
    "    if self.scoring_function=='dot':\n",
    "      # Intialize variables needed for Dot score function here\n",
    "        #self.similarity = []\n",
    "        self.softmax = Softmax(axis=1)\n",
    "        #self.similarity = [j for j in range(att_units)]\n",
    "        pass\n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    \n",
    "    if self.scoring_function == 'dot':\n",
    "        # Implement Dot score function here\n",
    "        #print(decoder_hidden_state.shape,encoder_output.shape)\n",
    "        attention_weight = tf.matmul(encoder_output,tf.expand_dims(decoder_hidden_state,axis=2))\n",
    "        #print(attention_weight.shape)\n",
    "        context = tf.matmul(tf.transpose(encoder_output, perm=[0,2,1]),attention_weight)\n",
    "        context = tf.squeeze(context,axis=2)\n",
    "        output = self.softmax(attention_weight)\n",
    "        return context,output\n",
    "    \n",
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        super().__init__()\n",
    "        self.tar_vocab_size = tar_vocab_size\n",
    "        self.lstm_size = dec_units\n",
    "        self.att_units = att_units\n",
    "        self.score_fun = score_fun\n",
    "        #print(\"output vocan size \",tar_vocab_size)\n",
    "        self.embedding = Embedding(input_dim=tar_vocab_size, output_dim=300, input_length=input_length,\n",
    "                           mask_zero=True,name=\"embedding_layer_encoder\",weights=[text_embed_matrix], trainable=False)\n",
    "        self.lstmcell = tf.keras.layers.LSTMCell(dec_units)\n",
    "        self.decoder_lstm = RNN(self.lstmcell,return_sequences=True, return_state=True)\n",
    "        self.dense   = Dense(tar_vocab_size)\n",
    "        #self.decoder_lstm = LSTM(lstm_size, return_state=True, return_sequences=True, name=\"decoder_LSTM\")\n",
    "        self.attention=Attention(self.score_fun,self.att_units)\n",
    "\n",
    "\n",
    "    def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "\n",
    "        output2 = self.embedding(input_to_decoder)\n",
    "        mask = self.embedding.compute_mask(input_to_decoder)\n",
    "        #print(\"one step decoder SHAPE after embedding:\",output2.shape)\n",
    "        output2 = tf.squeeze(output2,axis=1)\n",
    "        #print(\"one step decoder SHAPE after embedding and sqeezing:\",output2.shape)\n",
    "\n",
    "        # step b\n",
    "    #         attention=Attention(self.score_fun,self.att_units)\n",
    "        context_vector,attention_weights=self.attention(state_h,encoder_output)\n",
    "        # step c\n",
    "        output3 = tf.concat([context_vector,output2],1)\n",
    "        #print(\"shape after concating \",output3.shape)\n",
    "        output3 = tf.expand_dims(output3,1)\n",
    "        deco_output, deco_state_h, deco_state_c = self.decoder_lstm(output3,initial_state=[state_h,state_c],mask=mask)\n",
    "        # step e\n",
    "        output4 = self.dense(deco_output)\n",
    "        output4 = tf.squeeze(output4,axis=1)\n",
    "        #print(\"shape afyer dense layer and softmax \",output4.shape)\n",
    "        return output4,deco_state_h, deco_state_c,attention_weights,context_vector\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "        super().__init__()\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.att_units = att_units\n",
    "        self.input_length = input_length\n",
    "        self.score_fun = score_fun\n",
    "        self.onestepdecoder = One_Step_Decoder(self.out_vocab_size,self.embedding_dim,self.input_length,self.dec_units,self.score_fun,self.att_units)\n",
    "        \n",
    "    @tf.function    \n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state):\n",
    "\n",
    "\n",
    "        all_outputs = tf.TensorArray(tf.float32,size=input_to_decoder.shape[1])\n",
    "        for timestep in range(input_to_decoder.shape[1]):\n",
    "            output,decoder_hidden_state,decoder_cell_state,attention_weights,context_vector=self.onestepdecoder(input_to_decoder[:,timestep:timestep+1],encoder_output,decoder_hidden_state,decoder_cell_state)\n",
    "            all_outputs = all_outputs.write(timestep,output)\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.transpose(all_outputs.stack(),[1,0,2])\n",
    "        #print(\"all outpt shape is \",all_outputs.shape)\n",
    "        return all_outputs\n",
    "    \n",
    "class EncoderDecoder(tf.keras.Model):\n",
    "    def __init__(self,inp_vocab_size,out_vocab_size,embedding_size,lstm_size,input_length,batch_size,score_fun,att_units,*args):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "        #print(\"input vocab size in encoder decoder class\",inp_vocab_size)\n",
    "        self.encoder = Encoder(inp_vocab_size,embedding_size,lstm_size,input_length)\n",
    "        #print(\"output vocab size in encoder decoder class\",out_vocab_size)\n",
    "        self.decoder = Decoder(out_vocab_size,embedding_size,input_length,lstm_size,score_fun,att_units)\n",
    "        self.dense   = Dense(out_vocab_size, activation='softmax')\n",
    "        self.flatten = Flatten()\n",
    "        self.batch = batch_size\n",
    "\n",
    "    def call(self,data):\n",
    "        input,output = data[0], data[1]\n",
    "        #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "        l = self.encoder.initialize_states(self.batch)\n",
    "        #print(\"WE ARE INITIALIZING encoder WITH initial STATES as zeroes :\",l[0].shape, l[1].shape)\n",
    "        encoder_output,encoder_final_state_h,encoder_final_state_c = self.encoder(input,l)\n",
    "        decoder_output = self.decoder(output,encoder_output,encoder_final_state_h,encoder_final_state_c)\n",
    "        decoder_output  = self.dense(decoder_output, activation='softmax')\n",
    "        decoder_output=self.flatten()\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "019e108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_func(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_value = loss_obj(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_value.dtype)\n",
    "    loss_value *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f981859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vocab_size = len(perturbated_text_embed_matrix) \n",
    "out_vocab_size = len(text_embed_matrix) \n",
    "embedding_dim=300\n",
    "inp_length=20\n",
    "lstm_size=64\n",
    "batch_size=256\n",
    "score_fun = \"dot\"\n",
    "att_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4b290d41",
   "metadata": {
    "id": "4b290d41"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n",
    "log_directory = os.getcwd() + '/attention_model/logs/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d966c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(inp_vocab_size,out_vocab_size,embedding_dim,lstm_size,input_length,batch_size,score_fun,att_units)\n",
    "model.compile(optimizer=optimizer,loss=loss_func, metrics=['accuracy'])\n",
    "model.train_on_batch([perturbated_text_train[:batch_size],text_inp_train[:batch_size]],text_out_train[:batch_size])\n",
    "model.save_weights('attention_model_weights', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7b3135f7",
   "metadata": {
    "id": "7b3135f7"
   },
   "outputs": [],
   "source": [
    "checkpoint_model = ModelCheckpoint(\"seq2seq_model_checkpoint.h5\", monitor='loss', save_best_only=True, save_weights_only=True, verbose=0, mode='min')\n",
    "earlystopping_model = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=log_directory, histogram_freq=1)\n",
    "callbacks_model = [checkpoint_model, tensorboard, earlystopping_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fe2f3e8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe2f3e8b",
    "outputId": "e0c043af-041c-4548-a523-18b354540b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_18 (Encoder)         multiple                  10471340  \n",
      "_________________________________________________________________\n",
      "decoder_14 (Decoder)         multiple                  12736634  \n",
      "=================================================================\n",
      "Total params: 23,207,974\n",
      "Trainable params: 2,451,874\n",
      "Non-trainable params: 20,756,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b73710bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b73710bb",
    "outputId": "44ec8b1b-c5a2-4295-a9c4-e278d5ffe10a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " 15/640 [..............................] - ETA: 28:02 - loss: 4.6563 - accuracy: 0.0492"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13584/2529528136.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperturbated_text_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_inp_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_out_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcallbacks_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=[perturbated_text_train,text_inp_train],y=text_out_train, epochs=60,batch_size=batch_size, callbacks=[callbacks_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4114ee1",
   "metadata": {
    "id": "d4114ee1"
   },
   "outputs": [],
   "source": [
    "#%tensorboard --log_dir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68a2dc",
   "metadata": {
    "id": "ef68a2dc"
   },
   "outputs": [],
   "source": [
    "def EncoderOutput(encoder_input):\n",
    "    enc_input=list()\n",
    "    for word in encoder_input.split():\n",
    "        if perturbated_text_tokernizer_index.get(word) != None:\n",
    "            enc_input.append(perturbated_text_tokernizer_index.get(word))\n",
    "        else:\n",
    "            enc_input.append(0)\n",
    "            \n",
    "    enc_output, enc_hidden_state, enc_cell_state = model.layers[0](np.array([enc_input], dtype='int32'))\n",
    "    \n",
    "    return enc_output, enc_hidden_state, enc_cell_state\n",
    "\n",
    "def PredictOutput(encoder_input, decoder_input):\n",
    "\n",
    "    dec_input=list()\n",
    "    for word in decoder_input.split():\n",
    "        if text_inp_tokernizer_word_index.get(word) != None:\n",
    "            dec_input.append(text_inp_tokernizer_word_index.get(word))\n",
    "        else:\n",
    "            dec_input.append(0)\n",
    "    \n",
    "    enc_output, enc_hidden_state, enc_cell_state=EncoderOutput(encoder_input)\n",
    "    \n",
    "    pred = model.layers[2](model.layers[1](np.array([dec_input], dtype='int32'),\n",
    "                                                                  enc_hidden_state, enc_cell_state))\n",
    "    transalated_output=\"\"\n",
    "    for word in pred[0]:\n",
    "        word = text_inp_tokernizer.index_word[tf.argmax(word).numpy()]\n",
    "        transalated_output += word + \" \"\n",
    "    \n",
    "    return transalated_output\n",
    "\n",
    "def InferResults(data):\n",
    "    output = []\n",
    "\n",
    "    for enc_inp,dec_inp, dec_out in data.values:\n",
    "        pred = PredictOutput(enc_inp,dec_inp)\n",
    "        output.append(pred)\n",
    "\n",
    "    data['correct_output'] = data['dec_out']\n",
    "    data['predicted_output'] = output\n",
    "\n",
    "    data = data.drop(['dec_inp', 'dec_out'], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def compute_blue_score(results):\n",
    "    data_output_bleu_score = []\n",
    "    for encoder_input_data, correct_output , predicted_output in results.values:\n",
    "        correct_output = correct_output.split()\n",
    "        predicted_output = predicted_output.rstrip().split()\n",
    "        if len(correct_output) == len(predicted_output):\n",
    "            data_output_bleu_score.append(sentence_bleu([correct_output],predicted_output))\n",
    "          \n",
    "    blue_score=sum(data_output_bleu_score)/len(data_output_bleu_score)\n",
    "    return blue_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45aa805",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "a45aa805",
    "outputId": "f7c9a8e3-1433-4fc4-a154-f1dad623fed2"
   },
   "outputs": [],
   "source": [
    "sample_data_train=train_data.sample(1000)\n",
    "results=InferResults(sample_data_train)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rv9y3_l2UCm6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rv9y3_l2UCm6",
    "outputId": "909c3148-fd67-40a8-c2f0-bb1d6401925b"
   },
   "outputs": [],
   "source": [
    "results.values[-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2IrP7WhMOkRf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IrP7WhMOkRf",
    "outputId": "4bcbc8fd-b82c-42a1-a775-1dd29efed147"
   },
   "outputs": [],
   "source": [
    "blue_score=compute_blue_score(results)\n",
    "print('BLUE score for Training data is: {}'.format(blue_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9A1F-aE0k2ow",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "9A1F-aE0k2ow",
    "outputId": "8f021b11-5a13-403a-c362-0f0340cddc85"
   },
   "outputs": [],
   "source": [
    "sample_data_test=test_data.sample(1000)\n",
    "results=InferResults(sample_data_test)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CGAN7hnyk2rw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGAN7hnyk2rw",
    "outputId": "b98d6651-79d7-4266-f21a-181858ec8b9f"
   },
   "outputs": [],
   "source": [
    "blue_score=compute_blue_score(results)\n",
    "print('BLUE score for Test data is: {}'.format(blue_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aYpb-gc8j3Oa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "aYpb-gc8j3Oa",
    "outputId": "1d6486b3-7692-4daf-b2b2-dccd2589dc60"
   },
   "outputs": [],
   "source": [
    "sample_data_validation=validation_data.sample(1000)\n",
    "results=InferResults(sample_data_validation)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lv0n7KvFkBmL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lv0n7KvFkBmL",
    "outputId": "fec03098-7431-4e18-91cc-4c3bc7de03f6"
   },
   "outputs": [],
   "source": [
    "blue_score=compute_blue_score(results)\n",
    "print('BLUE score for Validation data is: {}'.format(blue_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E0RO9iRfD8Mi",
   "metadata": {
    "id": "E0RO9iRfD8Mi"
   },
   "source": [
    "## Observations\n",
    "- The basic seq2seq model or basic encoder decoder model is proven to be ok.\n",
    "- The model gave good BLUE score of 0.75 on Train data, 0.437 on Test data and 0.44 on validation data \n",
    "- We need to build bit complex models by introducing attention mechanism in the base model to improve the accuracy drastically"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
