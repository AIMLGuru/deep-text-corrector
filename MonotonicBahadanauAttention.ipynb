{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a36d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, RNN, Flatten, Softmax\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder- Takes a input sequence and returns output sequence\n",
    "    '''\n",
    "    def __init__(self,input_vocab_size,embedd_size,lstm_size,inp_len,perturbated_text_embed_matrix):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embedd_size = embedd_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.inp_len = inp_len\n",
    "        self.perturbated_text_embed_matrix=perturbated_text_embed_matrix\n",
    "\n",
    "        self.embedd = Embedding(input_dim = self.input_vocab_size, output_dim = self.embedd_size, input_length=self.inp_len,\n",
    "                                weights = [self.perturbated_text_embed_matrix], mask_zero=True)\n",
    "        self.encoder_lstm = LSTM(units = self.lstm_size, return_sequences=True, return_state=True, \n",
    "                               name=\"Encoder\", kernel_regularizer= regularizers.l2(1e-5))\n",
    "\n",
    "    def call(self,inp_seq, training=True):\n",
    "        embedds = self.embedd(inp_seq)\n",
    "        encoder_output, encoder_hidden_state, encod_cell_state = self.encoder_lstm(embedds)\n",
    "        return encoder_output, encoder_hidden_state, encod_cell_state\n",
    "\n",
    "    def initialize_states(self,batch_size):\n",
    "        h_state = np.zeros((batch_size, self.lstm_units))\n",
    "        c_state = np.zeros((batch_size, self.lstm_units))\n",
    "        return h_state, c_state\n",
    "    \n",
    "############################################################################################\n",
    "#https://github.com/UdiBhaskar/TfKeras-Custom-Layers/blob/master/Seq2Seq/clayers.py\n",
    "class MonotonicBahadanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units,\n",
    "                 return_aweights=False,\n",
    "                 scaling_factor=None,\n",
    "                 noise_std=0,\n",
    "                 weights_initializer='he_normal',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        \n",
    "        if 'name' not in kwargs:\n",
    "            kwargs['name'] = \"\"\n",
    "            \n",
    "        super(MonotonicBahadanauAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.noise_std = noise_std\n",
    "        self.weights_initializer = initializers.get(weights_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.weights_regularizer = regularizers.l2(1e-2)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self._wa = layers.Dense(self.units, use_bias=False,\\\n",
    "            kernel_initializer=self.weights_initializer, bias_initializer=self.bias_initializer,\\\n",
    "                kernel_regularizer= self.weights_regularizer, name=self.name+\"Wa\")\n",
    "        \n",
    "        self._ua = layers.Dense(self.units,\\\n",
    "            kernel_initializer=self.weights_initializer, bias_initializer=self.bias_initializer,\\\n",
    "                kernel_regularizer= self.weights_regularizer, name=self.name+\"Ua\")\n",
    "        \n",
    "        self._va = layers.Dense(1, use_bias=False, kernel_initializer=self.weights_initializer,\\\n",
    "            kernel_regularizer= self.weights_regularizer,bias_initializer=self.bias_initializer, name=self.name+\"Va\")\n",
    "        \n",
    "        \n",
    "    def call(self, decoder_hidden_state, encoder_outputs, prev_attention, training=True):\n",
    "\n",
    "        encoder_outputs, decoder_hidden_state = tf.cast(encoder_outputs, tf.float32), \\\n",
    "            tf.cast(decoder_hidden_state, tf.float32)\n",
    "        \n",
    "        dec_hidden_with_time_axis = tf.expand_dims(decoder_hidden_state, 1)\n",
    "\n",
    "        weightwa=self._wa\n",
    "        weightua=self._ua\n",
    "        weightva=self._va\n",
    "        \n",
    "        #bahdanau attention score\n",
    "        score = weightva(tf.nn.tanh(weightwa(dec_hidden_with_time_axis) + weightua(encoder_outputs)))\n",
    "        score = tf.squeeze(score, [2])\n",
    "        \n",
    "        if self.scaling_factor is not None:\n",
    "            score = score/tf.sqrt(self.scaling_factor)\n",
    "\n",
    "        if training:\n",
    "            if self.noise_std > 0:\n",
    "                random_noise = tf.random.normal(shape=tf.shape(input=score), mean=0,\\\n",
    "                    stddev=self.noise_std, dtype=score.dtype, seed=self.seed)\n",
    "                score = score + random_noise\n",
    "\n",
    "        probabilities = tf.sigmoid(score)\n",
    "\n",
    "        #monotonic attention 'parallel' mode\n",
    "        cumprod_1mp_probabilities = tf.exp(tf.cumsum(tf.math.log(tf.clip_by_value(1-probabilities,\\\n",
    "            1e-10, 1)), axis=1, exclusive=True))\n",
    "        attention_weights = probabilities*cumprod_1mp_probabilities*tf.cumsum(prev_attention/\\\n",
    "            tf.clip_by_value(cumprod_1mp_probabilities, 1e-10, 1.), axis=1)\n",
    "        attention_weights = tf.expand_dims(attention_weights, 1)\n",
    "\n",
    "        context_vector = tf.matmul(attention_weights, encoder_outputs)\n",
    "        context_vector = tf.squeeze(context_vector, 1, name=\"context_vector\")\n",
    "\n",
    "        return context_vector, tf.squeeze(attention_weights, 1, name='attention_weights')\n",
    "    \n",
    "############################################################################################\n",
    "\n",
    "class OneStepDecoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,target_vocab_size, embedd_dim, inp_len, decoder_units, attention_units,text_embed_matrix):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.embedd_dim = embedd_dim\n",
    "        self.inp_len = inp_len\n",
    "        self.decoder_units = decoder_units\n",
    "        self.attention_units = attention_units\n",
    "        self.text_embed_matrix=text_embed_matrix\n",
    "        \n",
    "        self.decoder_embedding_layer = Embedding(input_dim = self.target_vocab_size, output_dim = self.embedd_dim,\n",
    "                                                 input_length = self.inp_len,\n",
    "                                      weights = [self.text_embed_matrix] , name=\"onestepdecoder_embedding_layer\", mask_zero=True)\n",
    "        \n",
    "        self.decoder_LSTM = LSTM(units = self.decoder_units, return_state=True, kernel_regularizer= regularizers.l2(1e-5))\n",
    "        \n",
    "        self.MonotonicBahadanauAttention=MonotonicBahadanauAttention(self.attention_units,\n",
    "                 return_aweights=False,\n",
    "                 scaling_factor=None,\n",
    "                 noise_std=0,\n",
    "                 weights_initializer='he_normal',\n",
    "                 bias_initializer='zeros',)\n",
    "\n",
    "        self.dense = Dense(units=self.target_vocab_size)\n",
    "\n",
    "    def call(self,inp_to_dec, encoder_output, state_hidden, state_cell, att_weights):\n",
    "        \n",
    "        decoder_embedd = self.decoder_embedding_layer(inp_to_dec)\n",
    "        context_vec, att_weights = self.MonotonicBahadanauAttention(state_hidden,encoder_output, att_weights)\n",
    "        decoder_embedd = tf.concat([tf.expand_dims(context_vec,1), decoder_embedd], axis=-1)\n",
    "        decoder_out, decoder_hidden_state, decoder_cell_state = self.decoder_LSTM(decoder_embedd, \n",
    "                                                                                initial_state=[state_hidden, state_cell])\n",
    "        onestep_decoder_output = self.dense(decoder_out)\n",
    "\n",
    "        return onestep_decoder_output, decoder_hidden_state, decoder_cell_state, att_weights, context_vec\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,output_vocab_size, embedd_dim, output_length, decoder_units , att_units):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedd_dim = embedd_dim\n",
    "        self.output_length = output_length\n",
    "        self.decoder_units = decoder_units\n",
    "        self.att_units = att_units\n",
    "        self.text_embed_matrix=text_embed_matrix\n",
    "\n",
    "        self.onestep_decoder = OneStepDecoder(self.output_vocab_size, self.embedd_dim, self.output_length, self.decoder_units\n",
    "                                              ,self.att_units,self.text_embed_matrix)\n",
    "\n",
    "        \n",
    "    def call(self, inp_to_dec,enc_out,decoder_h,decoder_c, att_weights ):\n",
    "\n",
    "        total_out = tf.TensorArray(tf.float32, size=tf.shape(inp_to_dec)[1], name='out_arrays')\n",
    "        i = tf.shape(inp_to_dec)[1]\n",
    "\n",
    "        for t_step in range(i):    \n",
    "\n",
    "            onestep_decoder_output, decoder_h, decoder_c, att_weights, context_vector = self.onestep_decoder(\n",
    "                                            inp_to_dec[:, t_step:t_step+1], enc_out, decoder_h, decoder_c, att_weights)\n",
    "\n",
    "            total_out = total_out.write(t_step, onestep_decoder_output)\n",
    "        \n",
    "        total_out = tf.transpose(total_out.stack(), [1,0,2])\n",
    "\n",
    "        return total_out\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "class bahadanau_attention_model(tf.keras.Model):\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, encoder_embedd_dim, decoder_embedd_dim, input_len, output_len, \n",
    "                 encoder_units, decoder_units, attention_units,perturbated_text_embed_matrix,text_embed_matrix):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.encoder_embedd_dim = encoder_embedd_dim\n",
    "        self.decoder_embedd_dim = decoder_embedd_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.encoder_units = encoder_units\n",
    "        self.decoder_units = decoder_units\n",
    "        self.attention_units = attention_units\n",
    "        self.perturbated_text_embed_matrix=perturbated_text_embed_matrix\n",
    "        self.text_embed_matrix=text_embed_matrix\n",
    "\n",
    "        self.encoder = Encoder(self.encoder_vocab_size,self.encoder_embedd_dim,self.encoder_units,self.input_len,\n",
    "                              self.perturbated_text_embed_matrix)\n",
    "        self.decoder = Decoder(self.decoder_vocab_size,self.decoder_embedd_dim,self.output_len,self.decoder_units,\n",
    "                               self.attention_units,self.text_embed_matrix)\n",
    "\n",
    "    def call(self, data, training=True):\n",
    "        encoder_inp, decoder_inp = data[0], data[1]\n",
    "        \n",
    "        encoder_out, encoder_h, encoder_c = self.encoder(encoder_inp)\n",
    "    \n",
    "        decoder_h = encoder_h  #initial decoder state is equal to final encoder hidden state\n",
    "        decoder_c = encoder_c\n",
    "\n",
    "        att_weights = np.zeros((512, 20), dtype='float32')\n",
    "        att_weights[:, 0] = 1\n",
    "        \n",
    "        final_output = self.decoder(decoder_inp,encoder_out,decoder_h,decoder_c, att_weights)\n",
    "        return final_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
